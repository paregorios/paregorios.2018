<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>paregorios.org (Posts about refine)</title><link>http://paregorios.org/</link><description></description><atom:link href="http://paregorios.org/categories/refine.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:paregorios@hotmail.com"&gt;Tom Elliott&lt;/a&gt; </copyright><lastBuildDate>Fri, 06 Apr 2018 23:11:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Using OpenRefine with Pleiades</title><link>http://paregorios.org/posts/2017/10/using-openrefine-with-pleiades/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;This past summer, &lt;a href="https://blogs.library.duke.edu/dcthree/"&gt;DC3&lt;/a&gt;'s &lt;a href="https://ryanfb.github.io/"&gt;Ryan Baumann&lt;/a&gt; developed a reconciliation service for &lt;i&gt;&lt;a href="https://pleiades.stoa.org/"&gt;Pleiades&lt;/a&gt;&lt;/i&gt;. He's named it &lt;i&gt;&lt;a href="http://geocollider-sinatra.herokuapp.com/"&gt;Geocollider&lt;/a&gt;&lt;/i&gt;. It has two manifestations:&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Upload &lt;a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000323.shtml"&gt;a CSV file&lt;/a&gt; containing placenames and/or longitude/latitude coordinates, set matching parameters, and get back a CSV file of possible matches.&lt;/li&gt;&lt;li&gt;An online Application Programming Interface (API) compatible with the &lt;i&gt;&lt;a href="http://openrefine.org/"&gt;OpenRefine&lt;/a&gt;&lt;/i&gt; data-cleaning tool.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;The first version is relatively self-documenting. This blog post is about using the second version with &lt;i&gt;OpenRefine&lt;/i&gt;.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;h3&gt;Reconciliation&lt;/h3&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;I.e., matching (collating, aligning) your placenames against places in &lt;i&gt;Pleiades&lt;/i&gt;.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Running &lt;i&gt;OpenRefine&lt;/i&gt; against &lt;i&gt;Geocollider&lt;/i&gt; for reconciliation purposes is as easy as:&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://openrefine.org/download.html"&gt;Download and install &lt;i&gt;OpenRefine&lt;/i&gt;&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Follow &lt;a href="https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation"&gt;the standard OpenRefine instructions for "Reconciliation,"&lt;/a&gt; but instead of picking the pre-installed "Wikidata Reconciliation Service," select the "Add standard service..." button and enter "http://geocollider-sinatra.herokuapp.com/reconcile" in the service URL dialog, then select the "Add Service" button.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;When you've worked through the results of your reconciliation process and selected matches, &lt;i&gt;OpenRefine&lt;/i&gt; will have added the corresponding &lt;a href="https://pleiades.stoa.org/help/what-are-pleiades-uris"&gt;&lt;i&gt;Pleiades&lt;/i&gt; place URIs&lt;/a&gt; to your dataset. That may be all you want or need (for example, if you're preparing to bring your own dataset into &lt;a href="http://commons.pelagios.org/"&gt;the &lt;i&gt;Pelagios&lt;/i&gt; network&lt;/a&gt;) ... just export the results and go on with your work. &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;But if you'd like to actually get information &lt;b&gt;about&lt;/b&gt; the &lt;i&gt;Pleiades&lt;/i&gt; places, proceed to the next section.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;h3&gt;Augmentation&lt;/h3&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;I.e., pulling data from &lt;i&gt;Pleiades&lt;/i&gt; into &lt;i&gt;OpenRefine&lt;/i&gt; and selectively parsing it for information to add to your dataset.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;Pleiades&lt;/i&gt; provides an API for retrieving information about each place resource it contains. One of the data formats this API provides is &lt;a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000381.shtml"&gt;JSON&lt;/a&gt;, which is a format with which &lt;i&gt;OpenRefine&lt;/i&gt; is designed to work. The following recipe demonstrates how to use the &lt;a href="https://github.com/OpenRefine/OpenRefine/wiki/General-Refine-Expression-Language"&gt;General Refine Expression Language&lt;/a&gt; to extract the "Representative Location" associated with each &lt;i&gt;Pleiades&lt;/i&gt; place. &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="color: red;"&gt;&lt;b&gt;Caveat:&lt;/b&gt;&lt;/span&gt; this recipe will not, at present, work with the current Mac OSX release of &lt;i&gt;OpenRefine&lt;/i&gt; (2.7), even though it should and hopefully eventually will.  It has not been tested with the current releases for Windows and Linux, but they probably suffer from the same limitations as the OSX release. More information, including a non-trivial technical workaround, may be had from &lt;a href="https://github.com/OpenRefine/OpenRefine/issues/1265"&gt;OpenRefine Issue 1265&lt;/a&gt;. I will update this blog post if and when a resolution is forthcoming.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;b&gt;1. Create a new column containing &lt;i&gt;Pleiades&lt;/i&gt; JSON. &lt;/b&gt;&lt;br&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Assuming your dataset is open in an &lt;i&gt;OpenRefine&lt;/i&gt; project and that it contains a column that has been reconciled using &lt;i&gt;Geocollider&lt;/i&gt;, select the drop-down menu on that column and choose "Edit column" -&amp;gt; "Add column by fetching URLs ..."&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://4.bp.blogspot.com/-p7Op-SfHjOk/WeEyP9BNS_I/AAAAAAAAAvY/B7ndDjX2fuEEgS6H6c32Eu6tIRYfCUbDQCLcBGAs/s1600/foo1.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="Screen capture of OpenRefine column drop-down menu: add column by fetching URLs" border="0" data-original-height="378" data-original-width="409" height="369" src="https://4.bp.blogspot.com/-p7Op-SfHjOk/WeEyP9BNS_I/AAAAAAAAAvY/B7ndDjX2fuEEgS6H6c32Eu6tIRYfCUbDQCLcBGAs/s400/foo1.png" title="" width="400"&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;In the dialog box, provide a name for the new column you are about to create. In the "expression" box, enter a GREL expression that retrieves the &lt;i&gt;Pleiades&lt;/i&gt; URL from the reconciliation match on each cell and appends the string "/json" to it:&lt;/div&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;cell.recon.match.id + "/json"&lt;/span&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;blockquote class="tr_bq" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-vi2Jk_4TNqM/WeEyaD1MlTI/AAAAAAAAAvc/s1Zqp-crhog1b9yY2nN5h39DXgMM--zSgCLcBGAs/s1600/Screen%2BShot%2B2017-10-13%2Bat%2B4.18.59%2BPM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="Screen capture of OpenRefine dialog box: add column by fetching URLs" border="0" data-original-height="548" data-original-width="720" height="304" src="https://1.bp.blogspot.com/-vi2Jk_4TNqM/WeEyaD1MlTI/AAAAAAAAAvc/s1Zqp-crhog1b9yY2nN5h39DXgMM--zSgCLcBGAs/s400/Screen%2BShot%2B2017-10-13%2Bat%2B4.18.59%2BPM.png" title="" width="400"&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;div&gt;&lt;i&gt;OpenRefine&lt;/i&gt; retrieves the JSON for each matched place from &lt;i&gt;Pleiades&lt;/i&gt; and inserts it into the appropriate cell in the new column. &lt;/div&gt;&lt;br&gt;&lt;b&gt;2. Create another new column by parsing the representative longitude out of the JSON.&lt;/b&gt;&lt;br&gt;&lt;div&gt;&lt;br&gt;From the drop-down menu on the column containing JSON, select "Edit column" -&amp;gt; "Add column based on this column..."&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://3.bp.blogspot.com/-6Pp_ZLZop7E/WeEygxbwkSI/AAAAAAAAAvg/BhRqIf9VzLIuem68F86jLgQP7yfV40miwCLcBGAs/s1600/foo2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="Screen capture of OpenRefine column drop-down menu: add column based on this column" border="0" data-original-height="387" data-original-width="655" src="https://3.bp.blogspot.com/-6Pp_ZLZop7E/WeEygxbwkSI/AAAAAAAAAvg/BhRqIf9VzLIuem68F86jLgQP7yfV40miwCLcBGAs/s1600/foo2.png" title=""&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;In the dialog box, provide a name for the new column. In the expression box, enter a GREL expression that extracts the longitude from the &lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;reprPoint&lt;/span&gt; object in the JSON:&lt;/div&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;value.parseJson()['reprPoint'][0]&lt;/span&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://3.bp.blogspot.com/-eRYBqNDMXxg/WeEyrMvN9XI/AAAAAAAAAvk/nCkcYKP6zI0oMoeGI0Ek6_eoip4AbyHKwCLcBGAs/s1600/Screen%2BShot%2B2017-10-13%2Bat%2B4.30.13%2BPM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="Screen capture of OpenRefine column dialog box: add column based on this column" border="0" data-original-height="519" data-original-width="718" height="289" src="https://3.bp.blogspot.com/-eRYBqNDMXxg/WeEyrMvN9XI/AAAAAAAAAvk/nCkcYKP6zI0oMoeGI0Ek6_eoip4AbyHKwCLcBGAs/s400/Screen%2BShot%2B2017-10-13%2Bat%2B4.30.13%2BPM.png" title="" width="400"&gt;&lt;/a&gt;&lt;/div&gt;&lt;br&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Note that the &lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;reprPoint&lt;/span&gt; object contains a two-element list, like:&lt;/div&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;&lt;span style="white-space: pre-wrap;"&gt;[ &lt;/span&gt;&lt;span style="white-space: pre-wrap;"&gt;37.328382, &lt;/span&gt;&lt;span style="white-space: pre-wrap;"&gt;38.240638 ]&lt;/span&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;i&gt;Pleiades&lt;/i&gt; follows &lt;a href="https://tools.ietf.org/html/rfc7946"&gt;the GeoJSON specification&lt;/a&gt; in using the longitude, latitude ordering of elements in coordinate pairs so, to get the longitude, you use the index (0) for the first element in the list.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;b&gt;3. Create a column for the latitude&lt;/b&gt;&lt;/span&gt;&lt;br&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;Use the method explained in step 2, but select the second list item from &lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;reprPoint&lt;/span&gt; (index=1). &lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;b&gt;4. Carry on ...&lt;/b&gt;&lt;/span&gt;&lt;br&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;Your data set in &lt;i&gt;OpenRefine&lt;/i&gt; will now look something like this:  &lt;/span&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-O9NL6Rj0tT8/WeEywg8gBlI/AAAAAAAAAvs/vQ-aYkgXapg3lyzFWqayLL9Ka5OgW00CgCLcBGAs/s1600/Screen%2BShot%2B2017-10-13%2Bat%2B4.30.58%2BPM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="screen capture showing portion of an OpenRefine table that includes an ancient toponym, JSON retrieved from Pleiades, and latitude and longitude values extracted from that JSON" border="0" data-original-height="237" data-original-width="1082" src="https://1.bp.blogspot.com/-O9NL6Rj0tT8/WeEywg8gBlI/AAAAAAAAAvs/vQ-aYkgXapg3lyzFWqayLL9Ka5OgW00CgCLcBGAs/s1600/Screen%2BShot%2B2017-10-13%2Bat%2B4.30.58%2BPM.png" title=""&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;</description><category>csv</category><category>horothesia</category><category>interop</category><category>json</category><category>patterns</category><category>pelagoios</category><category>pleiades</category><category>refine</category><guid>http://paregorios.org/posts/2017/10/using-openrefine-with-pleiades/</guid><pubDate>Sat, 14 Oct 2017 03:49:00 GMT</pubDate></item><item><title>Mining AWOL for Identifiers</title><link>http://paregorios.org/posts/2014/04/mining-awol-for-identifiers/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;span style="color: red;"&gt;&lt;b&gt;NB: There is now a follow-up post to this one, in which various bad assumptions made here are addressed: "&lt;a href="http://horothesia.blogspot.com/2014/04/mining-awol-more-carefully-for-issns.html"&gt;Mining AWOL more carefully for ISSNs&lt;/a&gt;". &lt;/b&gt;&lt;/span&gt;&lt;br&gt;&lt;br&gt;In collaboration with &lt;a href="https://www.linkedin.com/in/pavanatri"&gt;Pavan Artri&lt;/a&gt;, &lt;a href="http://isaw.nyu.edu/people/staff/dawn-gross"&gt;Dawn Gross&lt;/a&gt;, &lt;a href="http://www.libraries.psu.edu/psul/staffdirectory.html?uid=cej14"&gt;Chuck Jones&lt;/a&gt;, &lt;a href="https://www.linkedin.com/pub/ronak-parpani/42/195/241"&gt;Ronak Parpani&lt;/a&gt;, and &lt;a href="http://isaw.nyu.edu/about/news/david-ratzan-becomes-isaws-head-of-library"&gt;David Ratzan&lt;/a&gt;, I'm currently working on a project to port the content of&lt;a href="http://ancientworldonline.blogspot.com/"&gt; Chuck's &lt;i&gt;Ancient World Online (AWOL)&lt;/i&gt; blog&lt;/a&gt; to a &lt;a href="https://www.zotero.org/"&gt;Zotero&lt;/a&gt; library. Funded in part by a grant from the &lt;a href="http://delmas.org/"&gt;Gladys Krieble Delmas Foundation&lt;/a&gt;, the idea is to make the information Chuck gathers available for more structured data needs, like citation generation, creation of library catalog records, and participation in linked data graphs. So far, we have code that successfully parses the &lt;a href="http://en.wikipedia.org/wiki/Atom_%28standard%29"&gt;Atom XML&lt;/a&gt; "backup" file we can get from Blogger and uses the &lt;a href="https://www.zotero.org/support/dev/server_api/v2/start"&gt;Zotero API&lt;/a&gt; to create a Zotero record for each blog post and to populate its title (derived from the title of the post), url (the first link we find in the body of the post), and tags (pulled from the Blogger "labels").&lt;br&gt;&lt;br&gt;We know that some of the post bodies also contain standard numbers (like &lt;a href="http://en.wikipedia.org/wiki/International_Standard_Serial_Number"&gt;ISSNs&lt;/a&gt; and &lt;a href="http://en.wikipedia.org/wiki/International_Standard_Book_Number"&gt;ISBNs&lt;/a&gt;), but it has been unclear how many of them there are and how regular the structure of text strings in which they appear. Would it be worthwhile to try to mine them out programmatically and insert them into the Zotero records as well? If so, what's our best strategy for capturing them ... i.e., what sort of parenthetical remarks, whitespace, and punctuation might intervene between them and the corresponding values? Time to do some data prospecting ...&lt;br&gt;&lt;br&gt;We'd previously split the monolithic "backup" XML file into &lt;a href="https://github.com/paregorios/awol-backup"&gt;individual XML files, one per post &lt;/a&gt;(click at your own risk; there are a lot of files in that github listing and your browser performance in rendering the page and its JavaScript may vary). Rather than writing a script to parse all that stuff just to figure out what's going on, I decided to try my new favorite can-opener, &lt;a href="http://beyondgrep.com/"&gt;ack&lt;/a&gt; (previously installed stresslessly on my Mac with another great tool, the &lt;a href="http://brew.sh/"&gt;Homebrew package manager&lt;/a&gt;).&lt;br&gt;&lt;br&gt;Time for some fun with &lt;a href="http://en.wikipedia.org/wiki/Regular_expression"&gt;regular expressions&lt;/a&gt;! I worked on this part iteratively, trying to start out as liberally as possible, thereby letting in a lot of irrelevant stuff so as not to miss anything good. I assumed that we want to catch acronyms, so strings of two or more capital letters, preceded by a word boundary. I didn't want to just use a [A-Z] range, since AWOL indexes multilingual resources, so I had recourse to the &lt;a href="http://www.regular-expressions.info/unicode.html"&gt;Unicode Categories feature&lt;/a&gt; that's available in most modern regular expression engines, including recent versions of &lt;a href="http://en.wikipedia.org/wiki/Perl"&gt;Perl&lt;/a&gt; (on which ack relies). So, I started off with:&lt;br&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "Courier New",Courier,monospace;'&gt;\b\p{Lu}\p{Lu}+&lt;/span&gt;&lt;/blockquote&gt;After some iteration on the results, I ended up with something more complex, trying to capture anything that fell between the acronym itself and the first subsequent colon, which seemed to be the standard delimiter between the designation+explanation of the type of identifier and the identifying value itself. I figure we'll worry how to parse the value later, once we're sure which identifiers we want to capture. So, here's the regex I ultimately used:&lt;br&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "Courier New",Courier,monospace;'&gt;\b\p{Lu}\p{Lu}+[:\s][^\b\p{P}]*[\b\:]&lt;/span&gt;&lt;/blockquote&gt;The full ack command looked like this:&lt;br&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "Courier New",Courier,monospace;'&gt;ack -oh "\b\p{Lu}\p{Lu}+[:\s][^\b\p{P}]*[\b\:]" post-*.xml &amp;gt; ../awol-acronyms/raw.txt&lt;/span&gt;&lt;/blockquote&gt;where the &lt;span style='font-family: "Courier New",Courier,monospace;'&gt;-h&lt;/span&gt; option telling ack to "suppress the prefixing of filenames on output when multiple files are searched" and the &lt;span style='font-family: "Courier New",Courier,monospace;'&gt;-o&lt;/span&gt; option telling ack to "show only the part of each line matching" my regex pattern (quotes from the ack man page). &lt;a href="https://github.com/paregorios/awol-acronyms/blob/master/raw.txt"&gt;You can browse the raw results here&lt;/a&gt;.&lt;br&gt;&lt;br&gt;So, how to get this text file into a more analyzable state? First, I thought I'd pull it into my text editor, &lt;a href="http://www.sublimetext.com/"&gt;Sublime&lt;/a&gt;, and use its text manipulation functions to filter for unique lines and then sort them. But then, it occurred to me that I really wanted to know frequency of identifier classes across the whole of the blog content, so I turned to &lt;a href="http://openrefine.org/"&gt;OpenRefine&lt;/a&gt;.&lt;br&gt;&lt;br&gt;I followed OR's standard process for importing a text file (being sure to set the right character encoding for the file on which I was working). Then, I used the column edit functionality and the string manipulation functions in the Open Refine Expression Language (abbreviated GREL because it used to be called "Google Refine Expression Language") to clean up the strings (regularizing whitespace, trimming leading and trailing whitespace, converting everything to uppercase, and getting rid of whitespace immediately preceding colons). That part could all have been done in a step outside OR with other tools, but I didn't think about it until I was already there.&lt;br&gt;&lt;br&gt;Then came the part OR is actually good at, faceting the data (i.e., getting all the unique strings and counts of same). I then used the GREL facetCount() function to get those values into the table itself, &lt;a href="http://googlerefine.blogspot.com/2011/08/remove-duplicate.html"&gt;followed this recipe to get rid of matching rows in the data&lt;/a&gt;, and exported &lt;a href="https://github.com/paregorios/awol-acronyms/blob/master/facetcounts.csv"&gt;a CSV file of the unique terms and their counts&lt;/a&gt; (github's default display for CSV makes our initial column very wide, so you may have to click on the "raw" link to see all the columns of data).&lt;br&gt;&lt;br&gt;There are some things that need investigating, but what strikes me is that apparently only ISSN is probably worth capturing programmatically. ISSNs appear 44 times in 14 different variations:&lt;br&gt;&lt;br&gt;&lt;style&gt;table {  }td { padding-top: 1px; padding-right: 1px; padding-left: 1px; color: black; font-size: 12pt; font-weight: 400; font-style: normal; text-decoration: none; font-family: Calibri,sans-serif; vertical-align: bottom; border: medium none; white-space: nowrap; }&lt;/style&gt;    &lt;br&gt;&lt;table border="0" cellpadding="0" cellspacing="0" style="border-collapse: collapse; width: 130px;"&gt;  &lt;colgroup&gt;&lt;col span="2" style="width: 65pt;" width="65"&gt; &lt;/colgroup&gt;&lt;tbody&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt; width: 65pt;" width="65"&gt;ISSN:&lt;/td&gt;  &lt;td align="right" style="width: 65pt;" width="65"&gt;17&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN paper:&lt;/td&gt;  &lt;td align="right"&gt;9&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN electrònic:&lt;/td&gt;  &lt;td align="right"&gt;4&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN electronic edition:&lt;/td&gt;  &lt;td align="right"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN electrónico:&lt;/td&gt;  &lt;td align="right"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN électronique:&lt;/td&gt;  &lt;td align="right"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN impreso:&lt;/td&gt;  &lt;td align="right"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN Online:&lt;/td&gt;  &lt;td align="right"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN edición electrónica:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN format papier:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN Print:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN print edition:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ONLINE ISSN:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;PRINT ISSN:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;Compare ISBNs:&lt;br&gt;&lt;br&gt;&lt;style&gt;table {  }td { padding-top: 1px; padding-right: 1px; padding-left: 1px; color: black; font-size: 12pt; font-weight: 400; font-style: normal; text-decoration: none; font-family: Calibri,sans-serif; vertical-align: bottom; border: medium none; white-space: nowrap; }.xl63 { text-align: left; }&lt;/style&gt;    &lt;br&gt;&lt;table border="0" cellpadding="0" cellspacing="0" style="border-collapse: collapse; width: 130px;"&gt;  &lt;colgroup&gt;&lt;col style="width: 65pt;" width="65"&gt; &lt;col style="width: 65pt;" width="65"&gt; &lt;/colgroup&gt;&lt;tbody&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt; width: 65pt;" width="65"&gt;ISBN of Second Part:&lt;/td&gt;  &lt;td class="xl63" style="width: 65pt;" width="65"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISBN:&lt;/td&gt;  &lt;td class="xl63"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISBN Compiled by:&lt;/td&gt;  &lt;td class="xl63"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;a href="http://en.wikipedia.org/wiki/Digital_object_identifier"&gt;DOIs&lt;/a&gt; make only one appearance, and there are no Library of Congress cataloging numbers.&lt;br&gt;&lt;br&gt;Now to point my collaborators at this blog post and see if they agree with me... &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;</description><category>ack</category><category>atom</category><category>awol</category><category>blogs</category><category>efficiency</category><category>horothesia</category><category>LOD</category><category>patterns</category><category>refine</category><category>thpppt</category><category>xml</category><category>zotero</category><guid>http://paregorios.org/posts/2014/04/mining-awol-for-identifiers/</guid><pubDate>Fri, 11 Apr 2014 22:37:00 GMT</pubDate></item></channel></rss>