<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>paregorios.org (Posts about patterns)</title><link>http://paregorios.org/</link><description></description><atom:link href="http://paregorios.org/categories/patterns.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:paregorios@hotmail.com"&gt;Tom Elliott&lt;/a&gt; </copyright><lastBuildDate>Fri, 06 Apr 2018 23:11:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Using OpenRefine with Pleiades</title><link>http://paregorios.org/posts/2017/10/using-openrefine-with-pleiades/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;This past summer, &lt;a href="https://blogs.library.duke.edu/dcthree/"&gt;DC3&lt;/a&gt;'s &lt;a href="https://ryanfb.github.io/"&gt;Ryan Baumann&lt;/a&gt; developed a reconciliation service for &lt;i&gt;&lt;a href="https://pleiades.stoa.org/"&gt;Pleiades&lt;/a&gt;&lt;/i&gt;. He's named it &lt;i&gt;&lt;a href="http://geocollider-sinatra.herokuapp.com/"&gt;Geocollider&lt;/a&gt;&lt;/i&gt;. It has two manifestations:&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Upload &lt;a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000323.shtml"&gt;a CSV file&lt;/a&gt; containing placenames and/or longitude/latitude coordinates, set matching parameters, and get back a CSV file of possible matches.&lt;/li&gt;&lt;li&gt;An online Application Programming Interface (API) compatible with the &lt;i&gt;&lt;a href="http://openrefine.org/"&gt;OpenRefine&lt;/a&gt;&lt;/i&gt; data-cleaning tool.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;The first version is relatively self-documenting. This blog post is about using the second version with &lt;i&gt;OpenRefine&lt;/i&gt;.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;h3&gt;Reconciliation&lt;/h3&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;I.e., matching (collating, aligning) your placenames against places in &lt;i&gt;Pleiades&lt;/i&gt;.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Running &lt;i&gt;OpenRefine&lt;/i&gt; against &lt;i&gt;Geocollider&lt;/i&gt; for reconciliation purposes is as easy as:&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://openrefine.org/download.html"&gt;Download and install &lt;i&gt;OpenRefine&lt;/i&gt;&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Follow &lt;a href="https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation"&gt;the standard OpenRefine instructions for "Reconciliation,"&lt;/a&gt; but instead of picking the pre-installed "Wikidata Reconciliation Service," select the "Add standard service..." button and enter "http://geocollider-sinatra.herokuapp.com/reconcile" in the service URL dialog, then select the "Add Service" button.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;When you've worked through the results of your reconciliation process and selected matches, &lt;i&gt;OpenRefine&lt;/i&gt; will have added the corresponding &lt;a href="https://pleiades.stoa.org/help/what-are-pleiades-uris"&gt;&lt;i&gt;Pleiades&lt;/i&gt; place URIs&lt;/a&gt; to your dataset. That may be all you want or need (for example, if you're preparing to bring your own dataset into &lt;a href="http://commons.pelagios.org/"&gt;the &lt;i&gt;Pelagios&lt;/i&gt; network&lt;/a&gt;) ... just export the results and go on with your work. &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;But if you'd like to actually get information &lt;b&gt;about&lt;/b&gt; the &lt;i&gt;Pleiades&lt;/i&gt; places, proceed to the next section.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;h3&gt;Augmentation&lt;/h3&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;I.e., pulling data from &lt;i&gt;Pleiades&lt;/i&gt; into &lt;i&gt;OpenRefine&lt;/i&gt; and selectively parsing it for information to add to your dataset.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;Pleiades&lt;/i&gt; provides an API for retrieving information about each place resource it contains. One of the data formats this API provides is &lt;a href="https://www.loc.gov/preservation/digital/formats/fdd/fdd000381.shtml"&gt;JSON&lt;/a&gt;, which is a format with which &lt;i&gt;OpenRefine&lt;/i&gt; is designed to work. The following recipe demonstrates how to use the &lt;a href="https://github.com/OpenRefine/OpenRefine/wiki/General-Refine-Expression-Language"&gt;General Refine Expression Language&lt;/a&gt; to extract the "Representative Location" associated with each &lt;i&gt;Pleiades&lt;/i&gt; place. &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="color: red;"&gt;&lt;b&gt;Caveat:&lt;/b&gt;&lt;/span&gt; this recipe will not, at present, work with the current Mac OSX release of &lt;i&gt;OpenRefine&lt;/i&gt; (2.7), even though it should and hopefully eventually will.  It has not been tested with the current releases for Windows and Linux, but they probably suffer from the same limitations as the OSX release. More information, including a non-trivial technical workaround, may be had from &lt;a href="https://github.com/OpenRefine/OpenRefine/issues/1265"&gt;OpenRefine Issue 1265&lt;/a&gt;. I will update this blog post if and when a resolution is forthcoming.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;b&gt;1. Create a new column containing &lt;i&gt;Pleiades&lt;/i&gt; JSON. &lt;/b&gt;&lt;br&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Assuming your dataset is open in an &lt;i&gt;OpenRefine&lt;/i&gt; project and that it contains a column that has been reconciled using &lt;i&gt;Geocollider&lt;/i&gt;, select the drop-down menu on that column and choose "Edit column" -&amp;gt; "Add column by fetching URLs ..."&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://4.bp.blogspot.com/-p7Op-SfHjOk/WeEyP9BNS_I/AAAAAAAAAvY/B7ndDjX2fuEEgS6H6c32Eu6tIRYfCUbDQCLcBGAs/s1600/foo1.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="Screen capture of OpenRefine column drop-down menu: add column by fetching URLs" border="0" data-original-height="378" data-original-width="409" height="369" src="https://4.bp.blogspot.com/-p7Op-SfHjOk/WeEyP9BNS_I/AAAAAAAAAvY/B7ndDjX2fuEEgS6H6c32Eu6tIRYfCUbDQCLcBGAs/s400/foo1.png" title="" width="400"&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;In the dialog box, provide a name for the new column you are about to create. In the "expression" box, enter a GREL expression that retrieves the &lt;i&gt;Pleiades&lt;/i&gt; URL from the reconciliation match on each cell and appends the string "/json" to it:&lt;/div&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;cell.recon.match.id + "/json"&lt;/span&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;blockquote class="tr_bq" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-vi2Jk_4TNqM/WeEyaD1MlTI/AAAAAAAAAvc/s1Zqp-crhog1b9yY2nN5h39DXgMM--zSgCLcBGAs/s1600/Screen%2BShot%2B2017-10-13%2Bat%2B4.18.59%2BPM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="Screen capture of OpenRefine dialog box: add column by fetching URLs" border="0" data-original-height="548" data-original-width="720" height="304" src="https://1.bp.blogspot.com/-vi2Jk_4TNqM/WeEyaD1MlTI/AAAAAAAAAvc/s1Zqp-crhog1b9yY2nN5h39DXgMM--zSgCLcBGAs/s400/Screen%2BShot%2B2017-10-13%2Bat%2B4.18.59%2BPM.png" title="" width="400"&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;div&gt;&lt;i&gt;OpenRefine&lt;/i&gt; retrieves the JSON for each matched place from &lt;i&gt;Pleiades&lt;/i&gt; and inserts it into the appropriate cell in the new column. &lt;/div&gt;&lt;br&gt;&lt;b&gt;2. Create another new column by parsing the representative longitude out of the JSON.&lt;/b&gt;&lt;br&gt;&lt;div&gt;&lt;br&gt;From the drop-down menu on the column containing JSON, select "Edit column" -&amp;gt; "Add column based on this column..."&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://3.bp.blogspot.com/-6Pp_ZLZop7E/WeEygxbwkSI/AAAAAAAAAvg/BhRqIf9VzLIuem68F86jLgQP7yfV40miwCLcBGAs/s1600/foo2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="Screen capture of OpenRefine column drop-down menu: add column based on this column" border="0" data-original-height="387" data-original-width="655" src="https://3.bp.blogspot.com/-6Pp_ZLZop7E/WeEygxbwkSI/AAAAAAAAAvg/BhRqIf9VzLIuem68F86jLgQP7yfV40miwCLcBGAs/s1600/foo2.png" title=""&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;In the dialog box, provide a name for the new column. In the expression box, enter a GREL expression that extracts the longitude from the &lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;reprPoint&lt;/span&gt; object in the JSON:&lt;/div&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;value.parseJson()['reprPoint'][0]&lt;/span&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://3.bp.blogspot.com/-eRYBqNDMXxg/WeEyrMvN9XI/AAAAAAAAAvk/nCkcYKP6zI0oMoeGI0Ek6_eoip4AbyHKwCLcBGAs/s1600/Screen%2BShot%2B2017-10-13%2Bat%2B4.30.13%2BPM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="Screen capture of OpenRefine column dialog box: add column based on this column" border="0" data-original-height="519" data-original-width="718" height="289" src="https://3.bp.blogspot.com/-eRYBqNDMXxg/WeEyrMvN9XI/AAAAAAAAAvk/nCkcYKP6zI0oMoeGI0Ek6_eoip4AbyHKwCLcBGAs/s400/Screen%2BShot%2B2017-10-13%2Bat%2B4.30.13%2BPM.png" title="" width="400"&gt;&lt;/a&gt;&lt;/div&gt;&lt;br&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Note that the &lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;reprPoint&lt;/span&gt; object contains a two-element list, like:&lt;/div&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;&lt;span style="white-space: pre-wrap;"&gt;[ &lt;/span&gt;&lt;span style="white-space: pre-wrap;"&gt;37.328382, &lt;/span&gt;&lt;span style="white-space: pre-wrap;"&gt;38.240638 ]&lt;/span&gt;&lt;/span&gt;&lt;/blockquote&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;i&gt;Pleiades&lt;/i&gt; follows &lt;a href="https://tools.ietf.org/html/rfc7946"&gt;the GeoJSON specification&lt;/a&gt; in using the longitude, latitude ordering of elements in coordinate pairs so, to get the longitude, you use the index (0) for the first element in the list.&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;b&gt;3. Create a column for the latitude&lt;/b&gt;&lt;/span&gt;&lt;br&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;Use the method explained in step 2, but select the second list item from &lt;span style='font-family: "courier new" , "courier" , monospace;'&gt;reprPoint&lt;/span&gt; (index=1). &lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;b&gt;4. Carry on ...&lt;/b&gt;&lt;/span&gt;&lt;br&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;Your data set in &lt;i&gt;OpenRefine&lt;/i&gt; will now look something like this:  &lt;/span&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-O9NL6Rj0tT8/WeEywg8gBlI/AAAAAAAAAvs/vQ-aYkgXapg3lyzFWqayLL9Ka5OgW00CgCLcBGAs/s1600/Screen%2BShot%2B2017-10-13%2Bat%2B4.30.58%2BPM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="screen capture showing portion of an OpenRefine table that includes an ancient toponym, JSON retrieved from Pleiades, and latitude and longitude values extracted from that JSON" border="0" data-original-height="237" data-original-width="1082" src="https://1.bp.blogspot.com/-O9NL6Rj0tT8/WeEywg8gBlI/AAAAAAAAAvs/vQ-aYkgXapg3lyzFWqayLL9Ka5OgW00CgCLcBGAs/s1600/Screen%2BShot%2B2017-10-13%2Bat%2B4.30.58%2BPM.png" title=""&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;&lt;div&gt;&lt;span style="white-space: pre-wrap;"&gt;&lt;br&gt;&lt;/span&gt;&lt;/div&gt;</description><category>csv</category><category>horothesia</category><category>interop</category><category>json</category><category>patterns</category><category>pelagoios</category><category>pleiades</category><category>refine</category><guid>http://paregorios.org/posts/2017/10/using-openrefine-with-pleiades/</guid><pubDate>Sat, 14 Oct 2017 03:49:00 GMT</pubDate></item><item><title>Mining AWOL for Identifiers</title><link>http://paregorios.org/posts/2014/04/mining-awol-for-identifiers/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;span style="color: red;"&gt;&lt;b&gt;NB: There is now a follow-up post to this one, in which various bad assumptions made here are addressed: "&lt;a href="http://horothesia.blogspot.com/2014/04/mining-awol-more-carefully-for-issns.html"&gt;Mining AWOL more carefully for ISSNs&lt;/a&gt;". &lt;/b&gt;&lt;/span&gt;&lt;br&gt;&lt;br&gt;In collaboration with &lt;a href="https://www.linkedin.com/in/pavanatri"&gt;Pavan Artri&lt;/a&gt;, &lt;a href="http://isaw.nyu.edu/people/staff/dawn-gross"&gt;Dawn Gross&lt;/a&gt;, &lt;a href="http://www.libraries.psu.edu/psul/staffdirectory.html?uid=cej14"&gt;Chuck Jones&lt;/a&gt;, &lt;a href="https://www.linkedin.com/pub/ronak-parpani/42/195/241"&gt;Ronak Parpani&lt;/a&gt;, and &lt;a href="http://isaw.nyu.edu/about/news/david-ratzan-becomes-isaws-head-of-library"&gt;David Ratzan&lt;/a&gt;, I'm currently working on a project to port the content of&lt;a href="http://ancientworldonline.blogspot.com/"&gt; Chuck's &lt;i&gt;Ancient World Online (AWOL)&lt;/i&gt; blog&lt;/a&gt; to a &lt;a href="https://www.zotero.org/"&gt;Zotero&lt;/a&gt; library. Funded in part by a grant from the &lt;a href="http://delmas.org/"&gt;Gladys Krieble Delmas Foundation&lt;/a&gt;, the idea is to make the information Chuck gathers available for more structured data needs, like citation generation, creation of library catalog records, and participation in linked data graphs. So far, we have code that successfully parses the &lt;a href="http://en.wikipedia.org/wiki/Atom_%28standard%29"&gt;Atom XML&lt;/a&gt; "backup" file we can get from Blogger and uses the &lt;a href="https://www.zotero.org/support/dev/server_api/v2/start"&gt;Zotero API&lt;/a&gt; to create a Zotero record for each blog post and to populate its title (derived from the title of the post), url (the first link we find in the body of the post), and tags (pulled from the Blogger "labels").&lt;br&gt;&lt;br&gt;We know that some of the post bodies also contain standard numbers (like &lt;a href="http://en.wikipedia.org/wiki/International_Standard_Serial_Number"&gt;ISSNs&lt;/a&gt; and &lt;a href="http://en.wikipedia.org/wiki/International_Standard_Book_Number"&gt;ISBNs&lt;/a&gt;), but it has been unclear how many of them there are and how regular the structure of text strings in which they appear. Would it be worthwhile to try to mine them out programmatically and insert them into the Zotero records as well? If so, what's our best strategy for capturing them ... i.e., what sort of parenthetical remarks, whitespace, and punctuation might intervene between them and the corresponding values? Time to do some data prospecting ...&lt;br&gt;&lt;br&gt;We'd previously split the monolithic "backup" XML file into &lt;a href="https://github.com/paregorios/awol-backup"&gt;individual XML files, one per post &lt;/a&gt;(click at your own risk; there are a lot of files in that github listing and your browser performance in rendering the page and its JavaScript may vary). Rather than writing a script to parse all that stuff just to figure out what's going on, I decided to try my new favorite can-opener, &lt;a href="http://beyondgrep.com/"&gt;ack&lt;/a&gt; (previously installed stresslessly on my Mac with another great tool, the &lt;a href="http://brew.sh/"&gt;Homebrew package manager&lt;/a&gt;).&lt;br&gt;&lt;br&gt;Time for some fun with &lt;a href="http://en.wikipedia.org/wiki/Regular_expression"&gt;regular expressions&lt;/a&gt;! I worked on this part iteratively, trying to start out as liberally as possible, thereby letting in a lot of irrelevant stuff so as not to miss anything good. I assumed that we want to catch acronyms, so strings of two or more capital letters, preceded by a word boundary. I didn't want to just use a [A-Z] range, since AWOL indexes multilingual resources, so I had recourse to the &lt;a href="http://www.regular-expressions.info/unicode.html"&gt;Unicode Categories feature&lt;/a&gt; that's available in most modern regular expression engines, including recent versions of &lt;a href="http://en.wikipedia.org/wiki/Perl"&gt;Perl&lt;/a&gt; (on which ack relies). So, I started off with:&lt;br&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "Courier New",Courier,monospace;'&gt;\b\p{Lu}\p{Lu}+&lt;/span&gt;&lt;/blockquote&gt;After some iteration on the results, I ended up with something more complex, trying to capture anything that fell between the acronym itself and the first subsequent colon, which seemed to be the standard delimiter between the designation+explanation of the type of identifier and the identifying value itself. I figure we'll worry how to parse the value later, once we're sure which identifiers we want to capture. So, here's the regex I ultimately used:&lt;br&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "Courier New",Courier,monospace;'&gt;\b\p{Lu}\p{Lu}+[:\s][^\b\p{P}]*[\b\:]&lt;/span&gt;&lt;/blockquote&gt;The full ack command looked like this:&lt;br&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style='font-family: "Courier New",Courier,monospace;'&gt;ack -oh "\b\p{Lu}\p{Lu}+[:\s][^\b\p{P}]*[\b\:]" post-*.xml &amp;gt; ../awol-acronyms/raw.txt&lt;/span&gt;&lt;/blockquote&gt;where the &lt;span style='font-family: "Courier New",Courier,monospace;'&gt;-h&lt;/span&gt; option telling ack to "suppress the prefixing of filenames on output when multiple files are searched" and the &lt;span style='font-family: "Courier New",Courier,monospace;'&gt;-o&lt;/span&gt; option telling ack to "show only the part of each line matching" my regex pattern (quotes from the ack man page). &lt;a href="https://github.com/paregorios/awol-acronyms/blob/master/raw.txt"&gt;You can browse the raw results here&lt;/a&gt;.&lt;br&gt;&lt;br&gt;So, how to get this text file into a more analyzable state? First, I thought I'd pull it into my text editor, &lt;a href="http://www.sublimetext.com/"&gt;Sublime&lt;/a&gt;, and use its text manipulation functions to filter for unique lines and then sort them. But then, it occurred to me that I really wanted to know frequency of identifier classes across the whole of the blog content, so I turned to &lt;a href="http://openrefine.org/"&gt;OpenRefine&lt;/a&gt;.&lt;br&gt;&lt;br&gt;I followed OR's standard process for importing a text file (being sure to set the right character encoding for the file on which I was working). Then, I used the column edit functionality and the string manipulation functions in the Open Refine Expression Language (abbreviated GREL because it used to be called "Google Refine Expression Language") to clean up the strings (regularizing whitespace, trimming leading and trailing whitespace, converting everything to uppercase, and getting rid of whitespace immediately preceding colons). That part could all have been done in a step outside OR with other tools, but I didn't think about it until I was already there.&lt;br&gt;&lt;br&gt;Then came the part OR is actually good at, faceting the data (i.e., getting all the unique strings and counts of same). I then used the GREL facetCount() function to get those values into the table itself, &lt;a href="http://googlerefine.blogspot.com/2011/08/remove-duplicate.html"&gt;followed this recipe to get rid of matching rows in the data&lt;/a&gt;, and exported &lt;a href="https://github.com/paregorios/awol-acronyms/blob/master/facetcounts.csv"&gt;a CSV file of the unique terms and their counts&lt;/a&gt; (github's default display for CSV makes our initial column very wide, so you may have to click on the "raw" link to see all the columns of data).&lt;br&gt;&lt;br&gt;There are some things that need investigating, but what strikes me is that apparently only ISSN is probably worth capturing programmatically. ISSNs appear 44 times in 14 different variations:&lt;br&gt;&lt;br&gt;&lt;style&gt;table {  }td { padding-top: 1px; padding-right: 1px; padding-left: 1px; color: black; font-size: 12pt; font-weight: 400; font-style: normal; text-decoration: none; font-family: Calibri,sans-serif; vertical-align: bottom; border: medium none; white-space: nowrap; }&lt;/style&gt;    &lt;br&gt;&lt;table border="0" cellpadding="0" cellspacing="0" style="border-collapse: collapse; width: 130px;"&gt;  &lt;colgroup&gt;&lt;col span="2" style="width: 65pt;" width="65"&gt; &lt;/colgroup&gt;&lt;tbody&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt; width: 65pt;" width="65"&gt;ISSN:&lt;/td&gt;  &lt;td align="right" style="width: 65pt;" width="65"&gt;17&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN paper:&lt;/td&gt;  &lt;td align="right"&gt;9&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN electrònic:&lt;/td&gt;  &lt;td align="right"&gt;4&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN electronic edition:&lt;/td&gt;  &lt;td align="right"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN electrónico:&lt;/td&gt;  &lt;td align="right"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN électronique:&lt;/td&gt;  &lt;td align="right"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN impreso:&lt;/td&gt;  &lt;td align="right"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN Online:&lt;/td&gt;  &lt;td align="right"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN edición electrónica:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN format papier:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN Print:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISSN print edition:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ONLINE ISSN:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;PRINT ISSN:&lt;/td&gt;  &lt;td align="right"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;Compare ISBNs:&lt;br&gt;&lt;br&gt;&lt;style&gt;table {  }td { padding-top: 1px; padding-right: 1px; padding-left: 1px; color: black; font-size: 12pt; font-weight: 400; font-style: normal; text-decoration: none; font-family: Calibri,sans-serif; vertical-align: bottom; border: medium none; white-space: nowrap; }.xl63 { text-align: left; }&lt;/style&gt;    &lt;br&gt;&lt;table border="0" cellpadding="0" cellspacing="0" style="border-collapse: collapse; width: 130px;"&gt;  &lt;colgroup&gt;&lt;col style="width: 65pt;" width="65"&gt; &lt;col style="width: 65pt;" width="65"&gt; &lt;/colgroup&gt;&lt;tbody&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt; width: 65pt;" width="65"&gt;ISBN of Second Part:&lt;/td&gt;  &lt;td class="xl63" style="width: 65pt;" width="65"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISBN:&lt;/td&gt;  &lt;td class="xl63"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;tr height="15" style="height: 15.0pt;"&gt;  &lt;td height="15" style="height: 15.0pt;"&gt;ISBN Compiled by:&lt;/td&gt;  &lt;td class="xl63"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br&gt;&lt;a href="http://en.wikipedia.org/wiki/Digital_object_identifier"&gt;DOIs&lt;/a&gt; make only one appearance, and there are no Library of Congress cataloging numbers.&lt;br&gt;&lt;br&gt;Now to point my collaborators at this blog post and see if they agree with me... &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;</description><category>ack</category><category>atom</category><category>awol</category><category>blogs</category><category>efficiency</category><category>horothesia</category><category>LOD</category><category>patterns</category><category>refine</category><category>thpppt</category><category>xml</category><category>zotero</category><guid>http://paregorios.org/posts/2014/04/mining-awol-for-identifiers/</guid><pubDate>Fri, 11 Apr 2014 22:37:00 GMT</pubDate></item><item><title>Planet Atlantides grows up and gets its own user-agent string</title><link>http://paregorios.org/posts/2014/02/planet-atlantides-grows-up-and-gets-its/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;So, sobered by recent spelunking and bad-bot-chasing in various server logs and convicted by &lt;a href="http://pythonhosted.org//feedparser/http-useragent.html"&gt;sage advice&lt;/a&gt; that ought to be followed by everyone in the &lt;a href="http://pythonhosted.org//feedparser/"&gt;UniversalFeedParser documentation&lt;/a&gt;, I have customized the bot used on &lt;i&gt;Planet Atlantides&lt;/i&gt; for fetching web feeds so it identifies itself unambiguously to the web servers from which it requests those feeds.&lt;br&gt;&lt;br&gt;Here's the explanatory text I just posted to the&lt;i&gt; &lt;a href="http://planet.atlantides.org/"&gt;Planet Atlantides&lt;/a&gt;&lt;/i&gt;&lt;a href="http://planet.atlantides.org/"&gt; home page&lt;/a&gt;. Please let me know if you have suggestions or critiques.&lt;br&gt;&lt;/p&gt;&lt;blockquote class="tr_bq"&gt;&lt;h3&gt;Feed reading, bots, and user agents&lt;/h3&gt;As implied above, &lt;i&gt;Planet Atlantides&lt;/i&gt; uses Sam Ruby's "Venus" branch of the &lt;i&gt;Planet&lt;/i&gt; "river of news" feed reader. That code is written in the Python language and uses an earlier version of the &lt;a href="http://pythonhosted.org/feedparser/index.html"&gt;Universal Feed Reader&lt;/a&gt; library for fetching web feeds (RSS and Atom formats). Out of the box, its http requests use the feed parser's default &lt;a href="http://en.wikipedia.org/wiki/User_agent"&gt;user agent&lt;/a&gt; string, so your server logs will only have recorded "UniversalFeedParser/4.2-pre-274-svn +http://feedparser.org/" when our copy of the software pulled your feed in the past.&lt;b&gt; &lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;b&gt;Effective 27 February 2014, the &lt;i&gt;Planet Atlantides&lt;/i&gt; production version of the code now identifies itself with the following user agent string: "PlanetAtlantidesFeedBot/0.2 +http://planet.atlantides.org/".&lt;/b&gt; Production code runs on a machine with the IP address 66.35.62.81, and never runs more than once per hour. Apart for a one-time set of test episodes on 27 February 2014 itself, log entries recording our user agent string and a different IP address represent spoofing by a potential bad actor other than me and my automagical bot. You should nuke them from orbit; it's the only way to be sure. Note that from time-to-time, I may run test code from other IP addresses, but I will in future use the user agent string beginning with "PlanetAtlantidesTestBot" for such runs. You can expect them to be infrequent and irregular.&lt;br&gt;&lt;br&gt;Please email me if you have any questions about &lt;i&gt;Planet Atlantides&lt;/i&gt;, its bot, or these user agent strings. In particular, if you put something like "PlanetAtlantidesBot is messing up my site" in your subject line, I'll look at it and respond as quickly as I can.&lt;/blockquote&gt;</description><category>atlantis</category><category>bots</category><category>citizenship</category><category>feeds</category><category>horothesia</category><category>patterns</category><guid>http://paregorios.org/posts/2014/02/planet-atlantides-grows-up-and-gets-its/</guid><pubDate>Thu, 27 Feb 2014 20:55:00 GMT</pubDate></item><item><title>Citing Sources in Digital Annotations</title><link>http://paregorios.org/posts/2013/04/citing-sources-in-digital-annotations/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;I'm collaborating with other folks both in and outside &lt;a href="http://isaw.nyu.edu/"&gt;ISAW&lt;/a&gt; on a variety of digital scholarly projects in which &lt;a href="http://www.w3.org/DesignIssues/LinkedData.html"&gt;Linked Open Data&lt;/a&gt; is playing a big role. We're using the &lt;a href="http://en.wikipedia.org/wiki/Resource_Description_Framework"&gt;Resource Description Framework (RDF)&lt;/a&gt; to provide descriptive information for, and make cross-project assertions about, a variety of entities of interest and the data associated with them (places, people, themes/subjects, creative works, bibliographic items, and manuscripts and other text-bearing objects). So, for example, I can produce the following assertions in RDF (using the &lt;a href="http://www.w3.org/TeamSubmission/turtle/"&gt;Terse RDF Triple Language, or TuRTLe&lt;/a&gt;):&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;pre style="font-family: Andale Mono, Lucida Console, Monaco, fixed, monospace; color: #000000; background-color: #eee;font-size: 12px;border: 1px dashed #999999;line-height: 14px;padding: 5px; overflow: auto; width: 100%"&gt;&lt;code&gt;&amp;lt;http://syriaca.org/place/45&amp;gt; a &amp;lt;http://geovocab.org/spatial#Feature&amp;gt; ;&lt;br&gt;  rdfs:label "Serugh" ;&lt;br&gt;  rdfs:comment "An ancient city where Jacob of Serugh was bishop."@en ;&lt;br&gt;  foaf:primaryTopicOf &amp;lt;http://en.wikipedia.org/wiki/Suruç&amp;gt; ;&lt;br&gt;  owl:sameAs &amp;lt;http://pleiades.stoa.org/places/658405#this&amp;gt; .&lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;This means: 'There's a resource identified with the &lt;a href="http://en.wikipedia.org/wiki/Uniform_resource_identifier"&gt;Uniform Resource Identifier (URI)&lt;/a&gt; "http://syriaca.org/place/45" about which the following is asserted: &lt;ul&gt;&lt;li&gt;it is a "Feature" as defined in the &lt;a href="http://geovocab.org/spatial.html"&gt;NeoGeo Spatial Ontology&lt;/a&gt;;&lt;/li&gt;&lt;li&gt;the &lt;a href="http://www.w3.org/TR/rdf-schema/#ch_label"&gt;human-readable version of its name&lt;/a&gt; is "Serugh";&lt;/li&gt;&lt;li&gt;&lt;a href="http://www.w3.org/TR/rdf-schema/#ch_comment"&gt;a human-readable description&lt;/a&gt; (in the English language) of it is "An ancient city where Jacob of Serugh was bishop.";&lt;/li&gt;&lt;li&gt;it is &lt;a href="http://xmlns.com/foaf/spec/#term_isPrimaryTopicOf"&gt;the primary topic of a document&lt;/a&gt; that is identified by the URI "http://en.wikipedia.org/wiki/Suruç"; and&lt;/li&gt;&lt;li&gt;it is &lt;a href="http://www.w3.org/TR/owl-ref/#sameAs-def"&gt;the same resource as&lt;/a&gt; that identified by another URI: "http://pleiades.stoa.org/places/658405#this".'&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;(Folks familiar with what &lt;a href="http://sgillies.net/"&gt;Sean Gillies&lt;/a&gt; has done for the &lt;a href="https://github.com/isawnyu/pleiades-rdf"&gt;Pleiades RDF&lt;/a&gt; will recognize my debt to him in the what proceeds.) &lt;br&gt;&lt;br&gt;But there are plenty of cases in which just issuing a couple of &lt;a href="http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/#dfn-rdf-triple"&gt;triples&lt;/a&gt; to encode an assertion about something isn't sufficient; we need to be able to assign responsibility/origin for those assertions and to link them to supporting argument and evidence (i.e., standard scholarly citation practice). For this purpose, we're very pleased by the &lt;a href="http://www.openannotation.org/"&gt;Open Annotation Collaboration&lt;/a&gt;, whose &lt;i&gt;Open Annotation Data Model&lt;/i&gt; was recently updated and expanded in the form of a &lt;a href="http://www.openannotation.org/spec/core/"&gt;W3C Community Draft (8 February 2013)&lt;/a&gt; (the participants in &lt;a href="http://pelagios-project.blogspot.com/"&gt;Pelagios&lt;/a&gt; use basic OAC annotations to assert geographic relationships between their data and Pleiades places).&lt;br&gt;&lt;br&gt;&lt;br&gt;A basic OADM annotation uses a series of RDF triples to link together a "target" (the thing you want to make an assertion about) and a "body" (the content of your assertion). You can think of them as footnotes. The "target" is the range of text after which you put your footnote number (only in OADM you can add a footnote to any real, conceptual, or digital thing you can identify) and the "body" is the content of the footnote itself. &lt;a href="http://www.openannotation.org/spec/core/core.html#BodyTarget"&gt;The OADM draft formally explains this structure in section 2.1&lt;/a&gt;. This lets me add an annotation to the resource from our example above (the ancient city of Serugh) by using the URI "http://syriaca.org/place/45" as the target of an annotation) thus: &lt;br&gt;&lt;pre style="font-family: Andale Mono, Lucida Console, Monaco, fixed, monospace; color: #000000; background-color: #eee;font-size: 12px;border: 1px dashed #999999;line-height: 14px;padding: 5px; overflow: auto; width: 100%"&gt;&lt;code&gt;&amp;lt;http://syriaca.org/place/45/anno/desc6&amp;gt; a oa:Annotation ;&lt;br&gt;  oa:hasBody &amp;lt;http://syriaca.org/place/45/anno/desc6/body&amp;gt; ;&lt;br&gt;  oa:hasTarget &amp;lt;http://syriaca.org/place/45&amp;gt; ;&lt;br&gt;  oa:motivatedBy oa:describing ;&lt;br&gt;  oa:annotatedBy &amp;lt;http://syriaca.org/editors.xml#tcarlson&amp;gt; ;&lt;br&gt;  oa:annotatedAt "2013-04-03T00:00:01Z" ;&lt;br&gt;  oa:serializedBy &amp;lt;https://github.com/paregorios/srpdemo1/blob/master/xsl/place2ttl.xsl&amp;gt; ;&lt;br&gt;  oa:serializedAt "2013-04-17T13:35:05.771-05:00" .&lt;br&gt;&lt;br&gt;&amp;lt;http://syriaca.org/place/45/anno/desc6/body&amp;gt; a cnt:ContentAsText, dctypes:Text ;&lt;br&gt;  cnt:chars "an ancient town, formerly located near Sarug."@en ;&lt;br&gt;  dc:format "text/plain" ;&lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;I hope you'll forgive me for not spelling that all out in plain text, as all the syntax and terms are explained in the OADM. What I'm concerned about in this blog post is really what the OADM &lt;strong&gt;doesn't&lt;/strong&gt; explicitly tell me how to do, namely: show that the annotation body is actually a quotation from a published book. The verb oa:annotatedBy lets me indicate that the annotation itself was made (i.e., the footnote was written) by a resource identified by the URI "http://syriaca.org/editors.xml#tcarlson". If I'd given you a few more triples, you could have figured out that that resource is a real person named Thomas Carlson, who is one of the editors working on the &lt;a href="http://syriaca.org"&gt;Syriac Reference Portal project&lt;/a&gt;. But how do I indicate (as he very much wants to do because he's a responsible scholar and has no interest in plagiarizing anyone) that he's deliberately quoting a book called &lt;a href="http://www.worldcat.org/oclc/255043315"&gt;&lt;i&gt;The Scattered Pearls: A History of Syriac Literature and Sciences&lt;/i&gt;&lt;/a&gt;? Here's what I came up with (using terms from &lt;a href="http://purl.org/spar/cito/"&gt;Citation Typing Ontology&lt;/a&gt; and the &lt;a href="http://dublincore.org/documents/dcmi-terms/"&gt;DCMI Metadata Terms&lt;/a&gt;): &lt;br&gt;&lt;pre style="font-family: Andale Mono, Lucida Console, Monaco, fixed, monospace; color: #000000; background-color: #eee;font-size: 12px;border: 1px dashed #999999;line-height: 14px;padding: 5px; overflow: auto; width: 100%"&gt;&lt;code&gt;&amp;lt;http://syriaca.org/place/45/anno/desc7/body&amp;gt; a cnt:ContentAsText, dctypes:Text ;&lt;br&gt;  cnt:chars "a small town in the Mudar territory, between Ḥarran and Jarabulus. [Modern name, Suruç (tr.)]"@en ;&lt;br&gt;  dc:format "text/plain" ;&lt;br&gt;  cito:citesAsSourceDocument &amp;lt;http://www.worldcat.org/oclc/255043315&amp;gt; ;&lt;br&gt;  dcterms:biblographicCitation  "The Scattered Pearls: A History of Syriac Literature and Sciences, p. 558"@en .&lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;The addition of the triple containing cito:citesAsSourceDocument lets me make a machine-actionable link to the additional structured bibliographic data about the book that's available at Worldcat (but it doesn't say anything about page numbers!). The addition of the triple containing dcterms:bibliographicCitation lets me provide a human-readable citation. &lt;br&gt;&lt;br&gt;I'd love to have feedback on this approach from folks in the OAC, CITO, DCTERMS, and general linked data communities. Could I do better? Should I do something differently? &lt;br&gt;&lt;hr&gt;&lt;br&gt;The SRP team is currently evaluating a sample batch of such annotations, which you're also welcome to view. The &lt;a href="https://github.com/paregorios/srpdemo1/tree/master/rdf/place"&gt;RDF can be found here&lt;/a&gt;. These files are generated from &lt;a href="https://github.com/paregorios/srpdemo1/tree/master/xml/places"&gt;the TEI XML here&lt;/a&gt; using &lt;a href="https://github.com/paregorios/srpdemo1/blob/master/xsl/place2ttl.xsl"&gt;the XSLT here&lt;/a&gt;.&lt;br&gt;</description><category>bibliography</category><category>collaboration</category><category>concordia</category><category>data</category><category>horothesia</category><category>interop</category><category>isaw</category><category>lawdi</category><category>linkeddata</category><category>LOD</category><category>neogeography</category><category>patterns</category><category>pelagios</category><category>pleiades</category><category>rdf</category><category>tei</category><category>xml</category><category>xslt</category><guid>http://paregorios.org/posts/2013/04/citing-sources-in-digital-annotations/</guid><pubDate>Thu, 18 Apr 2013 23:35:00 GMT</pubDate></item><item><title>Planet Taygete update</title><link>http://paregorios.org/posts/2010/07/planet-taygete-update/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;I just learned, thanks to a blog post at the old blog, that the website and blog for the &lt;a href="http://digashkelon.com/"&gt;Leon Levy Expedition to Ashkelon&lt;/a&gt; has moved but not put any &lt;a href="http://en.wikipedia.org/wiki/URL_redirection"&gt;standard redirects&lt;/a&gt; in place at the old blog or feed. I've updated the subscription list for &lt;a href="http://planet.atlantides.org/taygete/"&gt;Taygete Atlantis&lt;/a&gt; to point at the new source, where I see there are a large number of posts (since early June) that I had missed.&lt;/p&gt;</description><category>atlantis</category><category>blogs</category><category>feeds</category><category>horothesia</category><category>patterns</category><guid>http://paregorios.org/posts/2010/07/planet-taygete-update/</guid><pubDate>Fri, 16 Jul 2010 17:23:00 GMT</pubDate></item><item><title>Semantic Web, Scholarly Resources for Antiquity and the Museum</title><link>http://paregorios.org/posts/2009/01/semantic-web-scholarly-resources-for/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;Our on-going work on geographically functional, cross-resource, machine-actionable citation(!) with the Web continues to get more interesting.&lt;br&gt;&lt;br&gt;The kickoff was, of course, the joint NEH/JISC grant that is (under the rubric of the &lt;a href="http://concordia.atlantides.org/"&gt;Concordia&lt;/a&gt; project) funding our look at this in collaboration with the &lt;a href="http://www.kcl.ac.uk/schools/humanities/depts/cch"&gt;Centre for Computing in the Humanities&lt;/a&gt; at King's College, London. Our two workshops (and lots of discussion with other parties in between) have led us through &lt;a href="http://horothesia.blogspot.com/2007/09/feeds-for-pleiades-data.html"&gt;KML&lt;/a&gt;, &lt;a href="http://horothesia.blogspot.com/2008/02/atomgeorss-for-interoperability.html"&gt;Atom+GeoRSS&lt;/a&gt;, citation vocabularies and &lt;a href="http://groups.google.com/groups/profile?show=more&amp;amp;enc_user=3woajxYAAADz2Iah2CYKpGhWfIfANf6qo4cocwWvDVg2RHsu8f1bCg&amp;amp;group=oai-ore"&gt;OAI/ORE&lt;/a&gt; on to &lt;a href="http://horothesia.blogspot.com/search/label/batlasids"&gt;Cool URIs&lt;/a&gt;, &lt;a href="http://sgillies.net/blog/785/linking-open-geographic-data/"&gt;Linked Data&lt;/a&gt;, &lt;a href="http://sgillies.net/blog/849/why-not-cidoc-crm-at-this-time/"&gt;CIDOC CRM&lt;/a&gt; and more.&lt;br&gt;&lt;br&gt;Traffic is now building on the &lt;a href="http://groups.google.com/group/gawd"&gt;Graph of Ancient World Data discussion group&lt;/a&gt; (e.g., &lt;a href="http://groups.google.com/group/gawd/browse_thread/thread/226f0e5f6fb64237"&gt;Sebastian Heath's post on coin hoard data&lt;/a&gt; at &lt;a href="http://nomisma.org/"&gt;nomisma.org&lt;/a&gt;). Yesterday, &lt;a href="http://sgillies.net/me"&gt;Sean Gillies&lt;/a&gt; rolled out &lt;a href="http://www.atlantides.org/trac/pleiades/changeset/1445"&gt;some changes to the Pleiades interface&lt;/a&gt; that provide &lt;a href="http://www.w3.org/TR/cooluris/#hashuri"&gt;#this endpoints&lt;/a&gt; for Pleiades places, so that Sebastian and others can make explicit reference either to the historical places themselves (non-information resources cited like &lt;a href="http://pleiades.stoa.org/places/639166#this"&gt;http://pleiades.stoa.org/places/639166#this&lt;/a&gt;) or our descriptions of them on the web (information resources, cited like &lt;a href="http://pleiades.stoa.org/places/639166/"&gt;http://pleiades.stoa.org/places/639166/&lt;/a&gt;).&lt;br&gt;&lt;br&gt;And then this afternoon I came across the latest Talis Semantic Web podcast, featuring &lt;a href="http://blogs.talis.com/nodalities/2009/01/koven-smith-talks-about-the-semantic-web-and-museums.php"&gt;Koven Smith on Semantic Web initiatives at the Metropolitan Museum of Art&lt;/a&gt;. 38 minutes well-spent. They're thinking about and exploring a number of the approaches and technologies we're interested in, but from a museum perspective. It would be interesting to discuss how these methods could be used to better bridge gaps between museums, field archaeologists, epigraphers, numismatists, papyrologists, prosopographers, historical geographers, librarians, archivists and the rest!&lt;/p&gt;</description><category>ancgeo</category><category>batlasids</category><category>concordia</category><category>epigraphy</category><category>horothesia</category><category>interop</category><category>papyrology</category><category>patterns</category><category>pleiades</category><category>prosopography</category><guid>http://paregorios.org/posts/2009/01/semantic-web-scholarly-resources-for/</guid><pubDate>Wed, 28 Jan 2009 01:08:00 GMT</pubDate></item><item><title>Smithsonian 2.0</title><link>http://paregorios.org/posts/2009/01/smithsonian-20/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;a href="http://smithsonian20.si.edu/"&gt;Smithsonian 2.0&lt;/a&gt;, a "gathering to re-imagine the Smithsonian in the digital age," is going on right now in Washington. You can follow the procedings via:&lt;br&gt;&lt;ul&gt;&lt;li&gt;The &lt;a href="http://smithsonian20.si.edu/discussion.html"&gt;Smithsonian 2.0 Blog&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://hashtags.org/search?query=%23si20&amp;amp;submit=Search"&gt;Tweets with the hashtag #si20&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;</description><category>conferences</category><category>horothesia</category><category>patterns</category><category>uptake</category><guid>http://paregorios.org/posts/2009/01/smithsonian-20/</guid><pubDate>Fri, 23 Jan 2009 21:03:00 GMT</pubDate></item><item><title>Digital Archimedes Palimpsest</title><link>http://paregorios.org/posts/2008/10/digital-archimedes-palimpsest/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;This just in:&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;Ten years ago today, a private American collector purchased the Archimedes Palimpsest. Since that time he has guided and funded the project to conserve, image, and study the manuscript. After ten years of work, involving the expertise and goodwill of an extraordinary number of people working around the world, the Archimedes Palimpsest Project has released its data. It is a historic dataset, revealing new texts from the ancient world. It is an integrated product, weaving registered images in many wavebands of light with XML transcriptions of the Archimedes and Hyperides texts that are spatially mapped to those images. It has pushed boundaries for the imaging of documents, and relied almost exclusively on current international standards. We hope that this dataset will be a persistent digital resource for the decades to come. We also hope it will be helpful as an example for others who are conducting similar work. It published under a Creative Commons 3.0 attribution license, to ensure ease of access and the potential for widespread use. A complete facsimile of the revealed palimpsested texts is available on Googlebooks as "The Archimedes Palimpsest." It is hoped that this is the first of many uses to which the data will be put.&lt;br&gt;&lt;br&gt;For information on the Archimedes Palimpsest Project, please visit:&lt;br&gt;&lt;a href="http://www.archimedespalimpsest.org/"&gt;www.archimedespalimpsest.org &lt;/a&gt;&lt;http: org=""&gt;&lt;br&gt;&lt;br&gt;For the dataset, please visit:&lt;br&gt;&lt;a href="http://www.archimedespalimpsest.net/"&gt;www.archimedespalimpsest.net &lt;/a&gt;&lt;http: net=""&gt;&lt;br&gt;&lt;br&gt;We have set up a discussion forum on the Archimedes Palimpsest Project. Any member can invite anybody else to join. If you want to become a member, please email:&lt;br&gt;&lt;br&gt;&lt;a href="mailto:wnoel@thewalters.org"&gt;wnoel@thewalters.org&lt;/a&gt;&lt;br&gt;&lt;br&gt;I would be grateful if you would circulate this to your friends and colleagues.&lt;br&gt;&lt;br&gt;Thank you very much&lt;br&gt;Will Noel&lt;br&gt;The Walters Art Museum&lt;br&gt;October 29th, 2008.&lt;/http:&gt;&lt;/http:&gt;&lt;/blockquote&gt;I found it a bit tricky to find the Google Books version of this, so &lt;a href="http://books.google.com/books?id=_zX8OG3QoF4C&amp;amp;pg=PT146&amp;amp;dq=The+Archimedes+Palimpsest&amp;amp;as_brr=1"&gt;here's the link&lt;/a&gt;.</description><category>digclass</category><category>horothesia</category><category>patterns</category><category>publications</category><category>rights</category><category>tei</category><guid>http://paregorios.org/posts/2008/10/digital-archimedes-palimpsest/</guid><pubDate>Wed, 29 Oct 2008 22:41:00 GMT</pubDate></item><item><title>The DH Stack(s)</title><link>http://paregorios.org/posts/2008/10/dh-stacks/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;Lots of interesting posts in the last couple of days about Digital Humanities skills, software and cyberinfrastructure initiatives:&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Sean issues some &lt;a href="http://sgillies.net/blog/819/second-guessing-project-bamboo/"&gt;service-oriented caveats for Project Bamboo&lt;/a&gt; and thinks about skillsets for DH-ready graduates&lt;/li&gt;&lt;li&gt;&lt;a href="http://digitalscholarship.wordpress.com/2008/10/18/digital-humanities-jobs/"&gt;Lisa surveys recent DH job postings&lt;/a&gt;, muses on DH 'fieldness' and visits the aspiring digital humanist's skillset too&lt;/li&gt;&lt;li&gt;&lt;a href="http://digitalhistoryhacks.blogspot.com/"&gt;Bill T.'s whole blog&lt;/a&gt; remains essential reading in this meme-space (e.g., see his recent "&lt;a href="http://digitalhistoryhacks.blogspot.com/2008/10/navigating-digital-history.html"&gt;Navigating Digital History&lt;/a&gt;")&lt;br&gt;&lt;/li&gt;&lt;li&gt;Liza provides an example of &lt;a href="http://blog.threepress.org/2008/10/14/corpus-toneelkritiek-interbellum/"&gt;TEI+Python+lxml in publishing primary sources online&lt;/a&gt; (the same general stack the Duke/Heidelberg/King's/Columbia/UNC/NYU team has been using in the rework of the Duke Databank of Documentary Papyri and Heidelberger Gesamtverzeichniss -- coming soon)&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;</description><category>bamboo</category><category>blogs</category><category>digclass</category><category>horothesia</category><category>jobs</category><category>patterns</category><category>tei</category><category>tools</category><category>uptake</category><guid>http://paregorios.org/posts/2008/10/dh-stacks/</guid><pubDate>Mon, 20 Oct 2008 19:52:00 GMT</pubDate></item><item><title>Why email a newsletter but not post it?</title><link>http://paregorios.org/posts/2008/09/why-email-newsletter-but-not-post-it/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;I have to confess publicly, at the risk of being thought rude, that I was dumbfounded to read &lt;a href="http://www.currentepigraphy.org/2008/09/13/bes-newsletter-19-released-but-not-online/#comment-7079"&gt;Polly Low's comment&lt;/a&gt; this morning:&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;The absence of the latest newsletter from the &lt;a href="http://www.csad.ox.ac.uk/BES/"&gt;&lt;abbr title="British Epigraphy Society"&gt;BES&lt;/abbr&gt; website&lt;/a&gt; is deliberate — only [listserv] subscribers get the cutting-edge news!&lt;/blockquote&gt;Why on earth would a professional academic organization with a web presence and a mission statement thereon that contains the following words limit themselves in this way?:&lt;br&gt;&lt;blockquote&gt;to promote the study of inscriptions, texts and historical documents ... disseminating news of the latest developments in epigraphic studies, in Britain and around the world&lt;br&gt;&lt;/blockquote&gt;Or is the real issue that "subscribers" = "members" and timely access to the newsletter is seen by the Society as an exclusive benefit of membership?</description><category>epigraphy</category><category>horothesia</category><category>patterns</category><category>subaudible</category><category>walled gardens</category><guid>http://paregorios.org/posts/2008/09/why-email-newsletter-but-not-post-it/</guid><pubDate>Wed, 17 Sep 2008 21:36:00 GMT</pubDate></item></channel></rss>