<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>paregorios.org (Posts about python)</title><link>https://paregorios.org/</link><description></description><atom:link href="https://paregorios.org/categories/python.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:ipse@paregorios.org"&gt;Tom Elliott&lt;/a&gt; 
&lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img
alt="Creative Commons License" style="border-width:0"
src="https://i.creativecommons.org/l/by/4.0/80x15.png" /&gt;&lt;/a&gt;
&lt;span xmlns:dct="http://purl.org/dc/terms/"
property="dct:title"&gt;&lt;strong&gt;&lt;em&gt;paregorios.org&lt;/em&gt;&lt;/strong&gt;&lt;/span&gt; by
&lt;a xmlns:cc="http://creativecommons.org/ns#" href="https://paregorios.org/me"
property="cc:attributionName" rel="cc:attributionURL"&gt;Tom Elliott&lt;/a&gt; is
licensed under a &lt;a rel="license"
href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons
Attribution 4.0 International License&lt;/a&gt;.
</copyright><lastBuildDate>Sun, 06 May 2018 12:25:46 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Hacking on Apache Log Files with Python</title><link>https://paregorios.org/posts/2014/07/hacking-on-apache-log-files-with-python/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;There are plenty of tools out there for doing web request analysis. I wanted to pull some information out of some &lt;a href="https://www.apache.org/"&gt;Apache&lt;/a&gt; log files without all that overhead. Here's what I did:&lt;br&gt;&lt;br&gt;I got &lt;a href="http://www.technomancy.org/"&gt;Rory McCann&lt;/a&gt;'s &lt;i&gt;apache-log-parser &lt;/i&gt;(after some googling; it's &lt;a href="https://github.com/rory/apache-log-parser"&gt;on Github&lt;/a&gt; and &lt;a href="https://pypi.python.org/pypi/apache-log-parser/"&gt;on pypi&lt;/a&gt;).  I set up a Python virtual environment using Doug Hellmann's &lt;i&gt;&lt;a href="http://virtualenvwrapper.readthedocs.org/en/latest/"&gt;virtualenvwrapper&lt;/a&gt;&lt;/i&gt;, activated it, and then used:&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;pre style="background-image: URL(http://2.bp.blogspot.com/_z5ltvMQPaa8/SjJXr_U2YBI/AAAAAAAAAAM/46OqEP32CJ8/s320/codebg.gif); background: #f0f0f0; border: 1px dashed #CCCCCC; color: black; font-family: arial; font-size: 12px; height: auto; line-height: 20px; overflow: auto; padding: 0px; text-align: left; width: 99%;"&gt;&lt;code style="color: black; word-wrap: normal;"&gt; pip install apache-log-parser  &lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;Since I'd never used &lt;i&gt;apache-log-parser&lt;/i&gt; before, I had to get familiar with it. I discovered that, to use it, I had to know the format string that Apache was using to log information for my site. This was a two-step process, figured out by reading &lt;a href="https://httpd.apache.org/docs/2.2/logs.html"&gt;the Log Files section of the Apache documentation&lt;/a&gt; and poking about with &lt;a href="https://en.wikipedia.org/wiki/Grep"&gt;grep&lt;/a&gt;.&lt;br&gt;&lt;br&gt;First, I searched in the Apache configuration files for the &lt;span style="background-color: #eeeeee; font-family: Courier New, Courier, monospace;"&gt;CustomLog&lt;/span&gt; directive that's associated with the virtual host I wanted to analyze. This gave me a 'nickname' for the log configuration. More spelunking in Apache config files -- this time in the main configuration file -- turned up the definition of that nickname (Apache uses the &lt;span style="background-color: #eeeeee; font-family: Courier New, Courier, monospace;"&gt;LogFormat&lt;/span&gt; directive for this purpose):&lt;br&gt;&lt;br&gt;&lt;pre style="background: #f0f0f0; border: 1px dashed #CCCCCC; color: black; font-family: arial; font-size: 12px; height: auto; line-height: 20px; overflow: auto; padding: 0px; text-align: left; width: 99%;"&gt;&lt;code style="color: black; word-wrap: normal;"&gt; $ cd /etc/apache2/  &lt;br&gt; $ grep CustomLog sites-enabled/foo.nowhere.org  &lt;br&gt;  CustomLog /var/log/apache2/foo.nowhere.org-access.log combined  &lt;br&gt; $ grep combined apache2.conf   &lt;br&gt; LogFormat "%h %l %u %t \"%r\" %&amp;gt;s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined  &lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;It's that &lt;span style="background-color: #eeeeee; font-family: Courier New, Courier, monospace;"&gt;LogFormat&lt;/span&gt; string that needs to be given to Rory's code so it knows how to parse your log files.&lt;br&gt;&lt;br&gt;After some experimenting in the Python interpreter to get a feel for the code and its capabilities, I wrote a few lines of my own to wrap the setup and file reading operations:&lt;br&gt;&lt;br&gt;&lt;pre style="background: #f0f0f0; border: 1px dashed #CCCCCC; color: black; font-family: arial; font-size: 12px; height: auto; line-height: 20px; overflow: auto; padding: 0px; text-align: left; width: 99%;"&gt;&lt;code style="color: black; word-wrap: normal;"&gt; #!/usr/bin/env python  &lt;br&gt; # -*- coding: utf-8 -*-  &lt;br&gt;   &lt;br&gt; import apache_log_parser  &lt;br&gt; import glob  &lt;br&gt; import logging  &lt;br&gt;   &lt;br&gt; # supported log file formats  &lt;br&gt; APACHE_COMBINED="%h %l %u %t \"%r\" %&amp;gt;s %b \"%{Referer}i\" \"%{User-Agent}i\""  &lt;br&gt; APACHE_COMMON="%h %l %u %t \"%r\" %&amp;gt;s %b"  &lt;br&gt;   &lt;br&gt; def gulp(log_file_path, pattern=APACHE_COMBINED):  &lt;br&gt;   """ import and parse log files """  &lt;br&gt;   log_data=[]  &lt;br&gt;   line_parser=apache_log_parser.make_parser(pattern)  &lt;br&gt;   for file_name in glob.glob(log_file_path):  &lt;br&gt;     logging.info("file_name: %s" % file_name)  &lt;br&gt;     file = open(file_name, 'r')  &lt;br&gt;     lines = file.readlines()  &lt;br&gt;     file.close()  &lt;br&gt;     logging.info(" read %s lines" % len(lines))  &lt;br&gt;     for line in lines:  &lt;br&gt;       line_data=line_parser(line)  &lt;br&gt;       log_data.append(line_data)  &lt;br&gt;   logging.info("total number of events parsed: %s" % len(log_data))  &lt;br&gt;   return log_data  &lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;For this particular server, I had multiple log files, but I wanted to have all the requests from all of them (parsed into dictionaries by Rory's code) in a single list for subsequent analysis. So, back to the Python interpreter:&lt;br&gt;&lt;br&gt;&lt;pre style="background: #f0f0f0; border: 1px dashed #CCCCCC; color: black; font-family: arial; font-size: 12px; height: auto; line-height: 20px; overflow: auto; padding: 0px; text-align: left; width: 99%;"&gt;&lt;code style="color: black; word-wrap: normal;"&gt; &amp;gt;&amp;gt;&amp;gt; import logging  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; logging.basicConfig(level=logging.INFO)  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; import loggulper  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; d = loggulper.gulp("/path/to/my/log/files/foo.nowhere.org-access.*")  &lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;I'll spare you the logging messages. This took several minutes. I ended up with about 1.5 million requests in the list. Real life intervened. How to save this data for later without having to run through the whole import process again? &lt;a href="https://docs.python.org/2/library/pickle.html#data-stream-format"&gt;Pickle&lt;/a&gt; it.&lt;br&gt;&lt;br&gt;&lt;pre style="background: #f0f0f0; border: 1px dashed #CCCCCC; color: black; font-family: arial; font-size: 12px; height: auto; line-height: 20px; overflow: auto; padding: 0px; text-align: left; width: 99%;"&gt;&lt;code style="color: black; word-wrap: normal;"&gt; &amp;gt;&amp;gt;&amp;gt; import cPickle as pickle  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; out_name = "logdata.pickle"  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; outf = open(out_name, 'w')  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; pickler = pickle.Pickler(outf, pickle.HIGHEST_PROTOCOL)  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; pickler.dump(d)  &lt;br&gt; &amp;lt;cPickle.Pickler object at 0x1044044e8&amp;gt;  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; outf.close()  &lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;The whole list was saved to a 933.3 MB file in just a few seconds (full disclosure: I have a solid-state drive). It was nearly as quick to read back in a couple of days later (new interpreter session and all):&lt;br&gt;&lt;br&gt;&lt;pre style="background: #f0f0f0; border: 1px dashed #CCCCCC; color: black; font-family: arial; font-size: 12px; height: auto; line-height: 20px; overflow: auto; padding: 0px; text-align: left; width: 99%;"&gt;&lt;code style="color: black; word-wrap: normal;"&gt; &amp;gt;&amp;gt;&amp;gt; import cPickle as pickle  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; in_name="logdata.pickle"  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; inf=open(in_name, 'r')  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; unpickler=pickle.Unpickler(inf)  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; d=unpickler.load()  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; len(d)  &lt;br&gt; 1522015  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; d[0].keys()  &lt;br&gt; ['status', 'request_header_referer', 'remote_user', 'request_header_user_agent__browser__family', 'request_header_user_agent__is_mobile', 'request_header_user_agent__browser__version_string', 'request_header_user_agent', 'request_http_ver', 'request_header_user_agent__os__version_string', 'remote_logname', 'time_recieved_isoformat', 'time_recieved', 'request_first_line', 'request_header_user_agent__os__family', 'request_method', 'request_url', 'remote_host', 'time_recieved_datetimeobj', 'response_bytes_clf']  &lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;It's important to notice at this point that the word "received" is misspelled "recieved" in keys in the dictionaries returned by &lt;i&gt;apache-log-parser&lt;/i&gt;. If you don't notice this early on, it will cause some amount of frustration.&lt;br&gt;&lt;br&gt;It turned out that my log data included events past the end of reporting period I'd been given (ending 31 May 2014), so I needed to filter out just those requests that fell within the reporting period. &lt;a href="https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions"&gt;Python list comprehensions&lt;/a&gt; to the rescue:&lt;br&gt;&lt;br&gt;&lt;pre style="background: #f0f0f0; border: 1px dashed #CCCCCC; color: black; font-family: arial; font-size: 12px; height: auto; line-height: 20px; overflow: auto; padding: 0px; text-align: left; width: 99%;"&gt;&lt;code style="color: black; word-wrap: normal;"&gt; &amp;gt;&amp;gt;&amp;gt; dates=[req['time_recieved_datetimeobj'] for req in d]  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; max(dates)  &lt;br&gt; datetime.datetime(2014, 7, 13, 11, 37, 31)  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; min(dates)  &lt;br&gt; datetime.datetime(2013, 7, 21, 3, 41, 26)  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; from datetime import datetime  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; d_relevant=[req for req in d if req['time_recieved_datetimeobj'] &amp;lt; datetime(2014,06,01)]  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; dates=[req['time_recieved_datetimeobj'] for req in d_relevant]  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; max(dates)  &lt;br&gt; datetime.datetime(2014, 5, 31, 23, 59, 17)  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; min(dates)  &lt;br&gt; datetime.datetime(2013, 7, 21, 3, 41, 26)  &lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;Now to separate requests made by self-identified bots and spiders from the rest of the traffic:&lt;br&gt;&lt;br&gt;&lt;pre style="background: #f0f0f0; border: 1px dashed #CCCCCC; color: black; font-family: arial; font-size: 12px; height: auto; line-height: 20px; overflow: auto; padding: 0px; text-align: left; width: 99%;"&gt;&lt;code style="color: black; word-wrap: normal;"&gt; &amp;gt;&amp;gt;&amp;gt; robots=[req for req in d_relevant if 'bot' in req['request_header_user_agent'].lower() or 'spider' in req['request_header_user_agent'].lower()]  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; len(robots)  &lt;br&gt; 848450  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; humans=[req for req in d_relevant if 'bot' not in req['request_header_user_agent'].lower() and 'spider' not in req['request_header_user_agent'].lower()]  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; len(humans)  &lt;br&gt; 486249  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; len(robots) + len(humans) == len(d_relevant)  &lt;br&gt; True  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; unique_bots=[]  &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; for bot in robots:  &lt;br&gt; ...   if bot['request_header_user_agent'] not in unique_bots:  &lt;br&gt; ...     unique_bots.append(bot['request_header_user_agent'])  &lt;br&gt; ...   &lt;br&gt; &amp;gt;&amp;gt;&amp;gt; len(unique_bots)  &lt;br&gt; 229&lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;Aside: yes, I know there could well still be automated agents in the "humans" list; I've only filtered out &lt;a href="https://en.wikipedia.org/wiki/User_agent#Format_for_automated_agents_.28bots.29"&gt;those that are not operated by the sneaky or the uninformed&lt;/a&gt;. Let's not worry about that issue for now.&lt;br&gt;&lt;br&gt;Stay tuned for the next installment, wherein I hope we actually learn something about how our server is being used.</description><category>analytics</category><category>apache</category><category>horothesia</category><category>logs</category><category>python</category><guid>https://paregorios.org/posts/2014/07/hacking-on-apache-log-files-with-python/</guid><pubDate>Tue, 15 Jul 2014 04:47:00 GMT</pubDate></item><item><title>Get a List of Machine Tags from the Flickr API</title><link>https://paregorios.org/posts/2013/08/get-list-of-machine-tags-from-flickr-api/</link><dc:creator>Tom Elliott</dc:creator><description>&lt;p&gt;I want to know what &lt;a href="http://horothesia.blogspot.com/2011/09/feeds-of-flickr-photos-depicting.html"&gt;Pleiades machine tags&lt;/a&gt; are in use on photos throughout Flickr (&lt;a href="http://code.flickr.net/2011/12/16/pleiades-a-guest-post/"&gt;more background here&lt;/a&gt;). I thought I'd learn how to ask for that information from the &lt;a href="http://www.flickr.com/services/api/"&gt;Flickr API&lt;/a&gt; via a script. I requested and got an API key (see &lt;a href="http://www.flickr.com/help/api/"&gt;http://www.flickr.com/help/api/&lt;/a&gt;). I set up a &lt;a href="https://pypi.python.org/pypi/virtualenv"&gt;Python virtual environment&lt;/a&gt; and &lt;a href="http://git-scm.com/"&gt;git&lt;/a&gt; repository for the project. I went looking for Python code that already implemented interaction with the API and settled (somewhat arbitrarily) on &lt;a href="http://stuvel.eu/flickrapi"&gt;Beej's Python Flickr API kit &lt;/a&gt;(now maintained by Sybren Stüvel). Then used &lt;span style="background-color: #cccccc; font-family: Courier New, Courier, monospace; font-size: x-small;"&gt;&lt;a href="https://pypi.python.org/pypi/pip"&gt;pip&lt;/a&gt; install flickrapi&lt;/span&gt; to get the package.&lt;br&gt;&lt;br&gt;Here's a command-line session running the script and showing its output:&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;pre style="background-color: #eeeeee; border: 1px dashed #999999; color: black; font-family: Andale Mono, Lucida Console, Monaco, fixed, monospace; font-size: 12px; line-height: 14px; overflow: auto; padding: 5px; width: 100%;"&gt;&lt;code&gt;(pleiades-flickr)darkstar:pleiades-flickr paregorios$ python src/listptags.py &lt;br&gt;pleiades:atteststo is used on 15 photos in Flickr&lt;br&gt;pleiades:denotes is used on 1 photos in Flickr&lt;br&gt;pleiades:depcits is used on 2 photos in Flickr&lt;br&gt;pleiades:depicts is used on 7229 photos in Flickr&lt;br&gt;pleiades:findspot is used on 2197 photos in Flickr&lt;br&gt;pleiades:finspot is used on 2 photos in Flickr&lt;br&gt;pleiades:foundat is used on 1 photos in Flickr&lt;br&gt;pleiades:observedat is used on 3 photos in Flickr&lt;br&gt;pleiades:origin is used on 225 photos in Flickr&lt;br&gt;pleiades:place is used on 970 photos in Flickr&lt;br&gt;pleiades:places is used on 19 photos in Flickr&lt;br&gt;pleiades:where is used on 119 photos in Flickr&lt;br&gt;(pleiades-flickr)darkstar:pleiades-flickr paregorios$ &lt;br&gt;&lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;Here's the code (&lt;a href="https://github.com/paregorios/flickrexp/blob/aff7931b5816d70250841335ba24f9ba8b6fb970/listptags.py"&gt;version at github&lt;/a&gt;):&lt;br&gt;&lt;br&gt;&lt;pre style="background-color: #eeeeee; border: 1px dashed #999999; color: black; font-family: Andale Mono, Lucida Console, Monaco, fixed, monospace; font-size: 12px; line-height: 14px; overflow: auto; padding: 5px; width: 100%;"&gt;&lt;code&gt;#!/usr/bin/env python&lt;br&gt;"""&lt;br&gt;A Flickr tag bot&lt;br&gt;"""&lt;br&gt;&lt;br&gt;import argparse&lt;br&gt;import flickrapi&lt;br&gt;import json&lt;br&gt;import logging as l&lt;br&gt;import os&lt;br&gt;import sys&lt;br&gt;import traceback&lt;br&gt;&lt;br&gt;from myflickr import API_KEY, NAMESPACE_DEFAULT&lt;br&gt;&lt;br&gt;SCRIPT_DESC = "poll machine tags from flickr"&lt;br&gt;&lt;br&gt;def main ():&lt;br&gt;    """ Unleash the bot! """&lt;br&gt;&lt;br&gt;    global args&lt;br&gt;    global l&lt;br&gt;&lt;br&gt;    flickr = flickrapi.FlickrAPI(API_KEY)&lt;br&gt;    resp = flickr.machinetags_getPairs(namespace=args.namespace, format="json")&lt;br&gt;    if resp[:14] == "jsonFlickrApi(":&lt;br&gt;        jstr = resp[14:-1]&lt;br&gt;        j = json.loads(jstr)&lt;br&gt;        ptags = [(p['_content'], p['usage']) for p in j['pairs']['pair']]&lt;br&gt;        for ptag in ptags:&lt;br&gt;            print "%s is used on %s photos in Flickr" % ptag&lt;br&gt;&lt;br&gt;&lt;br&gt;if __name__ == "__main__":&lt;br&gt;    try:&lt;br&gt;        parser = argparse.ArgumentParser(description=SCRIPT_DESC, formatter_class=argparse.ArgumentDefaultsHelpFormatter)&lt;br&gt;        parser.add_argument ("-n", "--namespace", default=NAMESPACE_DEFAULT, help="namespace to use in requesting machine tags")&lt;br&gt;        parser.add_argument ("-v", "--verbose", action="store_true", default=False, help="verbose output")&lt;br&gt;        args = parser.parse_args()&lt;br&gt;        if args.verbose:&lt;br&gt;            l.basicConfig(level=l.DEBUG)&lt;br&gt;        else:&lt;br&gt;            l.basicConfig(level=l.WARNING)&lt;br&gt;        main()&lt;br&gt;        sys.exit(0)&lt;br&gt;    except KeyboardInterrupt, e: # Ctrl-C&lt;br&gt;        raise e&lt;br&gt;    except SystemExit, e: # sys.exit()&lt;br&gt;        raise e&lt;br&gt;    except Exception, e:&lt;br&gt;        print "ERROR, UNEXPECTED EXCEPTION"&lt;br&gt;        print str(e)&lt;br&gt;        traceback.print_exc()&lt;br&gt;        os._exit(1)&lt;br&gt;&lt;br&gt;&lt;/code&gt;&lt;/pre&gt;&lt;br&gt;Comments, questions, and constructive criticism welcomed!&lt;br&gt;&lt;br&gt;</description><category>flickr</category><category>horothesia</category><category>machine tags</category><category>pleiades</category><category>python</category><guid>https://paregorios.org/posts/2013/08/get-list-of-machine-tags-from-flickr-api/</guid><pubDate>Thu, 22 Aug 2013 00:41:00 GMT</pubDate></item></channel></rss>